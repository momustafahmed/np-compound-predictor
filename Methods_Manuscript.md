# Methods

## Data Simulation and Synthetic Dataset Generation

The dataset used in this study comprises 2,000 synthetic natural product compounds specifically designed to simulate realistic bioactivity screening data encountered in drug discovery research. The synthetic dataset was generated to provide a controlled experimental environment while maintaining the statistical properties, distributions, and relationships characteristic of authentic natural product compound libraries. This approach enables transparent methodology development and algorithm comparison without concerns regarding data privacy, intellectual property restrictions, or access limitations that often accompany proprietary pharmaceutical datasets.

The data simulation process was designed to replicate the complexity and variability of real-world natural product screening campaigns. Categorical variables were generated to represent typical diversity in compound libraries: biological sources were distributed among plant, marine, fungi, and bacterial origins reflecting the natural distribution of bioactive natural products in screening libraries. Chemical class assignments encompassed six major structural categories commonly observed in natural product chemistry: alkaloids, terpenes, polyketides, flavonoids, peptides, and saponins. These classifications were assigned with probabilities approximating their relative abundance in natural product databases, with alkaloids and terpenes representing the most common classes. Assay types included three major therapeutic areas in drug discovery: anticancer, antibacterial, and antimalarial activities, distributed to simulate a typical multi-target screening campaign.

Continuous molecular descriptors were generated using probability distributions informed by analysis of publicly available natural product databases and literature values. Molecular weight values were drawn from a log-normal distribution centered at 350 Daltons with a standard deviation designed to produce a realistic range spanning approximately 150 to 1,000 Daltons, consistent with the molecular weight distribution of known bioactive natural products. Lipophilicity values (ALogP) were generated from a normal distribution with mean of 2.5 and standard deviation of 2.0, reflecting the amphiphilic nature of many bioactive natural products that must balance aqueous solubility with membrane permeability. Topological polar surface area (TPSA) values were simulated from a gamma distribution to produce the characteristic right-skewed distribution observed in natural products, with values predominantly ranging from 20 to 200 Ų. Hydrogen bond donor and acceptor counts were generated from Poisson distributions with means of 3 and 5 respectively, reflecting typical values for drug-like natural products. The fraction of sp3-hybridized carbons (Fsp3) was sampled from a beta distribution to constrain values between 0 and 1, with parameters selected to favor moderate values around 0.3-0.5, consistent with the partial saturation commonly observed in natural product scaffolds.

Biological activity data were generated to incorporate realistic dose-response relationships and activity patterns. Test dose concentrations (dose_μM) were sampled from a log-uniform distribution spanning 0.1 to 40 μM, representing typical concentration ranges used in high-throughput screening campaigns. The half maximal inhibitory concentration (IC50_μM) values were generated using a sophisticated multi-step process to create plausible structure-activity relationships. First, a baseline activity was assigned to each compound based on a weighted combination of its molecular descriptors, with particular emphasis on molecular weight, lipophilicity, and polar surface area, as these properties are known to correlate with biological activity. This baseline activity was then modulated by categorical variables representing source and chemical class, reflecting the empirical observation that certain natural product classes show enhanced activity against specific biological targets. Stochastic noise was added using a log-normal distribution to simulate the inherent variability in biological assays. Finally, a censoring mechanism was applied such that compounds with predicted IC50 values exceeding 200 μM (indicating weak or no activity) were assigned the value 200 μM, mimicking the common practice in screening campaigns of reporting inactive compounds at the maximum test concentration.

To enhance the realism of the synthetic dataset, missing values were deliberately introduced in a pattern consistent with real experimental data. Approximately 2.5% of TPSA values (50 entries) and 3.1% of Fsp3 values (62 entries) were randomly set to missing, simulating scenarios where computational descriptor calculations may fail for certain complex molecular structures or where experimental measurement uncertainties prevent reliable value assignment. These missing values were subsequently addressed through median imputation, which was selected for its robustness to the outliers commonly present in chemical datasets. The median value of each feature was calculated from the available non-missing data and used to replace missing entries, ensuring that the imputation process did not introduce bias from extreme values that might be present in molecular descriptor distributions. This imputation strategy maintains the central tendency of each feature while avoiding the distortions that mean imputation could introduce in skewed distributions commonly observed in molecular property data.

## Dataset Characteristics and Quality Assessment

The final synthetic dataset exhibits statistical properties consistent with real-world natural product screening data, including realistic distributions of molecular properties, plausible structure-activity relationships, and characteristic patterns of missing data. The simulation approach ensures full transparency of data generation methodology while providing a realistic testbed for machine learning algorithm development. All parameters used in the data generation process were selected based on published analyses of natural product databases and represent consensus values from the medicinal chemistry literature. This transparent simulation framework enables reproducible research and fair comparison of predictive modeling approaches without the confounding factors of proprietary data access or selection bias that can complicate analyses of real pharmaceutical datasets.

## Feature Engineering and Data Transformation

Extensive feature engineering was performed to capture complex relationships between molecular properties and biological activity. The binary target variable for classification was defined as compounds exhibiting an IC50 value less than 100 micromolar, a threshold commonly used in drug discovery to identify compounds with meaningful biological activity. This threshold was selected based on standard practice in medicinal chemistry where IC50 values below 100 μM typically indicate sufficient potency to warrant further investigation, while values at or above 200 μM (the censoring threshold in the simulated data) clearly indicate lack of activity. This threshold-based definition resulted in 545 active compounds (27.3%) and 1,455 inactive compounds (72.7%), representing a moderately imbalanced classification problem. To capture non-linear relationships in the dose-response data, logarithmic transformations were applied to both IC50 and dose values using the formula log10(value + 1), where the addition of 1 prevents issues with zero or near-zero values. A dose-to-IC50 ratio feature was engineered by dividing the test dose by the sum of IC50 and 1, providing a normalized measure of the relationship between testing conditions and compound potency. Molecular property ratios were calculated to capture important physicochemical relationships, including the molecular weight to TPSA ratio (MW/TPSA), which relates molecular size to polarity, and the hydrogen bond acceptor to donor ratio (HBA/HBD), which provides insights into the balance of hydrogen bonding capacity.

Drug-likeness assessment was implemented through calculation of Lipinski's Rule of Five violations. Each compound was evaluated against four criteria: molecular weight exceeding 500 Daltons, ALogP exceeding 5, hydrogen bond donors exceeding 5, and hydrogen bond acceptors exceeding 10. The total number of violations was summed to create a discrete feature ranging from 0 to 4, where lower values indicate better drug-like properties. A comprehensive drug-likeness score was additionally computed by evaluating five expanded criteria: molecular weight within the range of 150 to 500 Daltons, ALogP within the range of -0.4 to 5.6, TPSA within the range of 20 to 130 Ų, hydrogen bond donors not exceeding 5, and hydrogen bond acceptors not exceeding 10. Each satisfied criterion contributed 0.2 to the final score, resulting in a continuous drug-likeness metric ranging from 0 to 1, with higher values indicating better drug-like characteristics.

Categorical variables were converted to numerical representations using label encoding, a necessary preprocessing step for machine learning algorithms that require numerical input. The source variable was encoded with values 0 through 3 representing bacteria, fungi, marine, and plant sources respectively. The chemical class variable was encoded with values 0 through 5 for alkaloid, flavonoid, peptide, polyketide, saponin, and terpene classes. The assay type variable was encoded with values 0 through 2 representing antibacterial, anticancer, and antimalarial assays respectively. This encoding scheme preserves the categorical information while enabling its use in mathematical operations required by machine learning algorithms.

## Data Splitting and Standardization

The complete dataset of 2,000 compounds was randomly partitioned into training and test sets using an 80:20 split ratio, yielding 1,600 compounds for model training and 400 compounds for independent evaluation. The split was performed using stratified sampling based on the binary activity label to ensure that both training and test sets maintained the same proportion of active and inactive compounds as the original dataset (27.3% active). A fixed random seed of 42 was used for all random operations to ensure reproducibility of results across different computational runs. Following the train-test split, feature standardization was applied using the StandardScaler method, which transforms each feature to have zero mean and unit variance. The scaling parameters (mean and standard deviation) were computed exclusively from the training set and then applied to both training and test sets, preventing information leakage from the test set that could lead to overly optimistic performance estimates. This standardization process is particularly important for algorithms sensitive to feature scales, such as support vector machines and k-nearest neighbors, ensuring that features with larger numerical ranges do not dominate the learning process.

## Handling Class Imbalance

The moderate class imbalance present in the dataset, with approximately 72.7% inactive compounds and 27.3% active compounds, was addressed using the Synthetic Minority Over-sampling Technique (SMOTE). This algorithm generates synthetic examples of the minority class by interpolating between existing minority class instances in feature space. Specifically, for each minority class sample, SMOTE identifies its k nearest minority class neighbors (where k=5 by default), randomly selects one of these neighbors, and creates a synthetic sample along the line connecting the two samples in the feature space. This process was applied exclusively to the training data, increasing the number of active compound examples from 436 to 1,164, thereby achieving a balanced training set with equal representation of both classes (1,164 active and 1,164 inactive compounds). The test set remained unchanged to provide an unbiased evaluation on the original class distribution. The use of SMOTE rather than simple oversampling helps prevent overfitting by introducing variation in the synthetic examples rather than merely duplicating existing minority class samples. This approach has been shown to improve model performance on imbalanced datasets while maintaining good generalization to new data.

## Machine Learning Algorithms

Seven distinct machine learning algorithms were implemented to provide comprehensive model comparison and robust prediction of compound activity. Each algorithm was selected to represent different learning paradigms and to capture various aspects of the relationship between molecular features and biological activity.

Logistic Regression served as the baseline linear model, implementing L2 regularization to prevent overfitting. The maximum number of iterations was set to 1,000 to ensure convergence of the optimization algorithm. This model provides interpretable coefficients that indicate the direction and magnitude of each feature's influence on the predicted activity, making it valuable for understanding feature importance from a linear perspective.

Random Forest classifier was implemented as an ensemble method combining 200 decision trees. Each tree was constructed using bootstrap sampling of the training data and considering a random subset of features at each split point. The maximum depth of each tree was limited to 15 levels to balance model complexity with generalization capability. Minimum samples required for splitting an internal node was set to 5, and minimum samples required to be a leaf node was set to 2. These hyperparameters were selected to prevent overfitting while maintaining sufficient model complexity to capture non-linear relationships. The Random Forest algorithm provides feature importance measures based on the average decrease in impurity across all trees, offering insights into which molecular descriptors most strongly influence activity predictions.

XGBoost (Extreme Gradient Boosting) was employed as an advanced gradient boosting framework. This implementation used 200 boosting rounds with a maximum tree depth of 6 levels and a learning rate of 0.1. Regularization was incorporated through subsample and colsample_bytree parameters, both set to 0.8, meaning that each tree was trained on 80% of the samples and 80% of the features selected randomly. The evaluation metric was set to logarithmic loss (logloss) to optimize probability predictions. XGBoost's sequential ensemble approach, where each subsequent tree attempts to correct the errors of previous trees, often yields state-of-the-art performance on tabular datasets.

Gradient Boosting classifier was implemented using scikit-learn's implementation with 150 estimators, maximum tree depth of 5, learning rate of 0.1, and subsample ratio of 0.8. While similar in concept to XGBoost, this implementation provides an alternative gradient boosting approach that may capture different patterns in the data through its different optimization strategy and tree construction methodology.

Support Vector Machine (SVM) with radial basis function (RBF) kernel was configured to find the optimal hyperplane for separating active and inactive compounds in a high-dimensional feature space. The regularization parameter C was set to 1.0, balancing margin maximization with training error minimization. The gamma parameter controlling the kernel width was set to 'scale', automatically computing it as 1/(n_features × variance) to adapt to the feature space dimensionality. Probability estimates were enabled to provide not only class predictions but also confidence scores for each prediction.

K-Nearest Neighbors (KNN) classifier was implemented with k=7 neighbors and distance-weighted voting, where closer neighbors have greater influence on the prediction than more distant ones. This instance-based learning algorithm makes predictions by identifying the k most similar compounds in the training set and using their activity labels to predict the query compound's activity. The choice of k=7 provides a balance between capturing local patterns and maintaining stability against noisy examples.

Naive Bayes classifier, specifically the Gaussian Naive Bayes variant, was included to provide a probabilistic baseline model. This algorithm assumes that features follow Gaussian distributions and are conditionally independent given the class label. Despite these simplifying assumptions, Naive Bayes often performs well in practice and provides very fast training and prediction times, making it valuable for rapid prototyping and as a computational efficiency baseline.

## Model Training and Hyperparameter Selection

All models were trained using the balanced training set produced by SMOTE, containing 2,328 samples (1,164 active and 1,164 inactive compounds) with 18 features each. Each algorithm was implemented using scikit-learn (version 1.3.2) or XGBoost (version 2.0.3) libraries in Python 3.9.6. Training was performed on a single machine with all available CPU cores utilized through the n_jobs=-1 parameter where supported. Hyperparameters for each model were selected based on established best practices and preliminary experiments to balance model complexity with computational efficiency. The Random Forest and XGBoost models used 200 estimators, providing sufficient ensemble diversity while maintaining reasonable training times. Tree depth limits (15 for Random Forest, 6 for XGBoost, 5 for Gradient Boosting) were chosen to prevent overfitting while allowing sufficient model complexity to capture non-linear relationships. Learning rates for gradient boosting methods (0.1 for both XGBoost and Gradient Boosting) were selected to provide stable convergence without requiring excessive numbers of boosting rounds. All random operations used a fixed random seed of 42 to ensure reproducibility of model training and evaluation.

## Model Evaluation and Performance Metrics

Model performance was evaluated on the held-out test set of 400 compounds that maintained the original class distribution (109 active and 291 inactive compounds). Multiple evaluation metrics were computed to provide comprehensive assessment of classification performance. Accuracy was calculated as the proportion of correct predictions among all test samples, providing an overall measure of model correctness. Precision (positive predictive value) was computed as the proportion of predicted active compounds that were truly active, quantifying the reliability of positive predictions. Recall (sensitivity or true positive rate) was calculated as the proportion of actual active compounds that were correctly identified, measuring the model's ability to find all positive cases. F1-score was computed as the harmonic mean of precision and recall, providing a single metric that balances both measures and is particularly informative for imbalanced datasets.

For models capable of producing probability estimates (all except K-Nearest Neighbors without probability calibration), additional probabilistic metrics were computed. The area under the receiver operating characteristic curve (ROC-AUC) was calculated by varying the classification threshold and plotting the true positive rate against the false positive rate, with the area under this curve providing a threshold-independent measure of classification performance. Average precision, computed from the precision-recall curve, provides a summary measure that is particularly informative for imbalanced datasets as it focuses on the performance on the minority (active) class. Confusion matrices were constructed for each model, tabulating true positives, true negatives, false positives, and false negatives to provide detailed insight into the types of errors made by each classifier.

Feature importance analysis was performed for tree-based models (Random Forest, XGBoost, and Gradient Boosting) using the built-in feature importance measures that quantify the average contribution of each feature to the model's predictions across all trees. For Random Forest, importance was measured as the mean decrease in impurity (Gini importance) weighted by the number of samples reaching each node. For gradient boosting methods, importance was measured as the average gain (improvement in loss function) contributed by each feature across all trees. The top 20 most important features were identified and visualized for each tree-based model to provide insights into which molecular descriptors and engineered features most strongly influence activity predictions.

## Statistical Analysis and Model Comparison

Model performance metrics were compared across all seven algorithms to identify the best performing approach and to understand the relative strengths of different learning paradigms on this dataset. ROC curves for all models were plotted on a single graph to enable visual comparison of their discrimination capabilities across different classification thresholds. The models were ranked by F1-score as the primary metric, given its appropriateness for moderately imbalanced classification problems where both precision and recall are important. Statistical significance of performance differences was assessed qualitatively by examining the magnitude of metric differences and the consistency of performance patterns across multiple evaluation measures. The best performing model was identified based on achieving the highest F1-score while also demonstrating strong performance across other complementary metrics such as accuracy, precision, recall, and ROC-AUC.

## Software Implementation and Reproducibility

All data processing, feature engineering, model training, and evaluation were implemented in Python 3.9.6 using established scientific computing libraries. Data manipulation and analysis utilized pandas (version 2.1.4) and NumPy (version 1.26.2). Machine learning algorithms were implemented using scikit-learn (version 1.3.2) for Logistic Regression, Random Forest, Gradient Boosting, SVM, K-Nearest Neighbors, and Naive Bayes, and XGBoost (version 2.0.3) for the XGBoost classifier. The SMOTE implementation was provided by imbalanced-learn (version 0.11.0). Model serialization for saving and loading trained models utilized joblib (version 1.3.2). Visualization of results was performed using Plotly (version 5.18.0), Seaborn (version 0.13.0), and Matplotlib (version 3.8.2). An interactive web-based dashboard for data exploration, model comparison, and real-time predictions was developed using Streamlit (version 1.29.0), providing a user-friendly interface for interacting with the trained models and exploring the dataset.

The complete analysis pipeline was organized into modular Python scripts: a preprocessing module handling data loading, cleaning, and feature engineering; a models module implementing all machine learning algorithms and evaluation metrics; a training script orchestrating the full model training pipeline; and a Streamlit application providing interactive visualization and prediction capabilities. All code was version controlled and documented with comprehensive docstrings explaining the purpose and parameters of each function. Random seeds were fixed throughout the analysis to ensure reproducibility of all results. The computational environment was managed using Python virtual environments with all dependencies specified in a requirements file, enabling exact replication of the software environment. All analyses were performed on a local computing environment running macOS, with training times ranging from seconds for simple models like Naive Bayes to several minutes for complex ensemble methods like Random Forest and XGBoost on the full training dataset.
