# Methods

## Dataset Description and Preparation

The dataset used in this study comprises 2,000 synthetic natural product compounds designed to simulate bioactivity screening data commonly encountered in drug discovery research. Each compound entry contains comprehensive molecular descriptors and biological activity measurements. The synthetic nature of this dataset allows for controlled experimentation while maintaining realistic patterns observed in actual compound libraries. The dataset includes categorical variables describing the biological source of each compound (plant, marine, fungi, or bacteria), the chemical class classification (alkaloid, terpene, polyketide, flavonoid, peptide, or saponin), and the assay type used for testing (anticancer, antibacterial, or antimalarial). Continuous molecular descriptors include molecular weight (MW), calculated lipophilicity (ALogP), topological polar surface area (TPSA), number of hydrogen bond donors (HBD) and acceptors (HBA), and the fraction of sp3-hybridized carbons (Fsp3). Biological activity data consists of the test dose concentration in micromolar units (dose_μM) and the half maximal inhibitory concentration (IC50_μM). The original dataset contained missing values in several features, with 50 missing entries in TPSA values and 62 missing entries in Fsp3 values, representing approximately 2.5% and 3.1% of the total data points respectively. These missing values were imputed using median imputation strategy, which was selected for its robustness to outliers commonly present in chemical datasets. The median value of each feature was calculated from the available data and used to replace missing entries, ensuring that the imputation process did not introduce bias from extreme values that might be present in molecular descriptor distributions.

## Feature Engineering and Data Transformation

Extensive feature engineering was performed to capture complex relationships between molecular properties and biological activity. The binary target variable for classification was defined as compounds exhibiting an IC50 value less than 100 micromolar, a threshold commonly used in drug discovery to identify compounds with meaningful biological activity. This threshold-based definition resulted in 545 active compounds (27.3%) and 1,455 inactive compounds (72.7%), representing a moderately imbalanced classification problem. To capture non-linear relationships in the dose-response data, logarithmic transformations were applied to both IC50 and dose values using the formula log10(value + 1), where the addition of 1 prevents issues with zero or near-zero values. A dose-to-IC50 ratio feature was engineered by dividing the test dose by the sum of IC50 and 1, providing a normalized measure of the relationship between testing conditions and compound potency. Molecular property ratios were calculated to capture important physicochemical relationships, including the molecular weight to TPSA ratio (MW/TPSA), which relates molecular size to polarity, and the hydrogen bond acceptor to donor ratio (HBA/HBD), which provides insights into the balance of hydrogen bonding capacity.

Drug-likeness assessment was implemented through calculation of Lipinski's Rule of Five violations. Each compound was evaluated against four criteria: molecular weight exceeding 500 Daltons, ALogP exceeding 5, hydrogen bond donors exceeding 5, and hydrogen bond acceptors exceeding 10. The total number of violations was summed to create a discrete feature ranging from 0 to 4, where lower values indicate better drug-like properties. A comprehensive drug-likeness score was additionally computed by evaluating five expanded criteria: molecular weight within the range of 150 to 500 Daltons, ALogP within the range of -0.4 to 5.6, TPSA within the range of 20 to 130 Ų, hydrogen bond donors not exceeding 5, and hydrogen bond acceptors not exceeding 10. Each satisfied criterion contributed 0.2 to the final score, resulting in a continuous drug-likeness metric ranging from 0 to 1, with higher values indicating better drug-like characteristics.

Categorical variables were converted to numerical representations using label encoding, a necessary preprocessing step for machine learning algorithms that require numerical input. The source variable was encoded with values 0 through 3 representing bacteria, fungi, marine, and plant sources respectively. The chemical class variable was encoded with values 0 through 5 for alkaloid, flavonoid, peptide, polyketide, saponin, and terpene classes. The assay type variable was encoded with values 0 through 2 representing antibacterial, anticancer, and antimalarial assays respectively. This encoding scheme preserves the categorical information while enabling its use in mathematical operations required by machine learning algorithms.

## Data Splitting and Standardization

The complete dataset of 2,000 compounds was randomly partitioned into training and test sets using an 80:20 split ratio, yielding 1,600 compounds for model training and 400 compounds for independent evaluation. The split was performed using stratified sampling based on the binary activity label to ensure that both training and test sets maintained the same proportion of active and inactive compounds as the original dataset (27.3% active). A fixed random seed of 42 was used for all random operations to ensure reproducibility of results across different computational runs. Following the train-test split, feature standardization was applied using the StandardScaler method, which transforms each feature to have zero mean and unit variance. The scaling parameters (mean and standard deviation) were computed exclusively from the training set and then applied to both training and test sets, preventing information leakage from the test set that could lead to overly optimistic performance estimates. This standardization process is particularly important for algorithms sensitive to feature scales, such as support vector machines and k-nearest neighbors, ensuring that features with larger numerical ranges do not dominate the learning process.

## Handling Class Imbalance

The moderate class imbalance present in the dataset, with approximately 72.7% inactive compounds and 27.3% active compounds, was addressed using the Synthetic Minority Over-sampling Technique (SMOTE). This algorithm generates synthetic examples of the minority class by interpolating between existing minority class instances in feature space. Specifically, for each minority class sample, SMOTE identifies its k nearest minority class neighbors (where k=5 by default), randomly selects one of these neighbors, and creates a synthetic sample along the line connecting the two samples in the feature space. This process was applied exclusively to the training data, increasing the number of active compound examples from 436 to 1,164, thereby achieving a balanced training set with equal representation of both classes (1,164 active and 1,164 inactive compounds). The test set remained unchanged to provide an unbiased evaluation on the original class distribution. The use of SMOTE rather than simple oversampling helps prevent overfitting by introducing variation in the synthetic examples rather than merely duplicating existing minority class samples. This approach has been shown to improve model performance on imbalanced datasets while maintaining good generalization to new data.

## Machine Learning Algorithms

Seven distinct machine learning algorithms were implemented to provide comprehensive model comparison and robust prediction of compound activity. Each algorithm was selected to represent different learning paradigms and to capture various aspects of the relationship between molecular features and biological activity.

Logistic Regression served as the baseline linear model, implementing L2 regularization to prevent overfitting. The maximum number of iterations was set to 1,000 to ensure convergence of the optimization algorithm. This model provides interpretable coefficients that indicate the direction and magnitude of each feature's influence on the predicted activity, making it valuable for understanding feature importance from a linear perspective.

Random Forest classifier was implemented as an ensemble method combining 200 decision trees. Each tree was constructed using bootstrap sampling of the training data and considering a random subset of features at each split point. The maximum depth of each tree was limited to 15 levels to balance model complexity with generalization capability. Minimum samples required for splitting an internal node was set to 5, and minimum samples required to be a leaf node was set to 2. These hyperparameters were selected to prevent overfitting while maintaining sufficient model complexity to capture non-linear relationships. The Random Forest algorithm provides feature importance measures based on the average decrease in impurity across all trees, offering insights into which molecular descriptors most strongly influence activity predictions.

XGBoost (Extreme Gradient Boosting) was employed as an advanced gradient boosting framework. This implementation used 200 boosting rounds with a maximum tree depth of 6 levels and a learning rate of 0.1. Regularization was incorporated through subsample and colsample_bytree parameters, both set to 0.8, meaning that each tree was trained on 80% of the samples and 80% of the features selected randomly. The evaluation metric was set to logarithmic loss (logloss) to optimize probability predictions. XGBoost's sequential ensemble approach, where each subsequent tree attempts to correct the errors of previous trees, often yields state-of-the-art performance on tabular datasets.

Gradient Boosting classifier was implemented using scikit-learn's implementation with 150 estimators, maximum tree depth of 5, learning rate of 0.1, and subsample ratio of 0.8. While similar in concept to XGBoost, this implementation provides an alternative gradient boosting approach that may capture different patterns in the data through its different optimization strategy and tree construction methodology.

Support Vector Machine (SVM) with radial basis function (RBF) kernel was configured to find the optimal hyperplane for separating active and inactive compounds in a high-dimensional feature space. The regularization parameter C was set to 1.0, balancing margin maximization with training error minimization. The gamma parameter controlling the kernel width was set to 'scale', automatically computing it as 1/(n_features × variance) to adapt to the feature space dimensionality. Probability estimates were enabled to provide not only class predictions but also confidence scores for each prediction.

K-Nearest Neighbors (KNN) classifier was implemented with k=7 neighbors and distance-weighted voting, where closer neighbors have greater influence on the prediction than more distant ones. This instance-based learning algorithm makes predictions by identifying the k most similar compounds in the training set and using their activity labels to predict the query compound's activity. The choice of k=7 provides a balance between capturing local patterns and maintaining stability against noisy examples.

Naive Bayes classifier, specifically the Gaussian Naive Bayes variant, was included to provide a probabilistic baseline model. This algorithm assumes that features follow Gaussian distributions and are conditionally independent given the class label. Despite these simplifying assumptions, Naive Bayes often performs well in practice and provides very fast training and prediction times, making it valuable for rapid prototyping and as a computational efficiency baseline.

## Model Training and Hyperparameter Selection

All models were trained using the balanced training set produced by SMOTE, containing 2,328 samples (1,164 active and 1,164 inactive compounds) with 18 features each. Each algorithm was implemented using scikit-learn (version 1.3.2) or XGBoost (version 2.0.3) libraries in Python 3.9.6. Training was performed on a single machine with all available CPU cores utilized through the n_jobs=-1 parameter where supported. Hyperparameters for each model were selected based on established best practices and preliminary experiments to balance model complexity with computational efficiency. The Random Forest and XGBoost models used 200 estimators, providing sufficient ensemble diversity while maintaining reasonable training times. Tree depth limits (15 for Random Forest, 6 for XGBoost, 5 for Gradient Boosting) were chosen to prevent overfitting while allowing sufficient model complexity to capture non-linear relationships. Learning rates for gradient boosting methods (0.1 for both XGBoost and Gradient Boosting) were selected to provide stable convergence without requiring excessive numbers of boosting rounds. All random operations used a fixed random seed of 42 to ensure reproducibility of model training and evaluation.

## Model Evaluation and Performance Metrics

Model performance was evaluated on the held-out test set of 400 compounds that maintained the original class distribution (109 active and 291 inactive compounds). Multiple evaluation metrics were computed to provide comprehensive assessment of classification performance. Accuracy was calculated as the proportion of correct predictions among all test samples, providing an overall measure of model correctness. Precision (positive predictive value) was computed as the proportion of predicted active compounds that were truly active, quantifying the reliability of positive predictions. Recall (sensitivity or true positive rate) was calculated as the proportion of actual active compounds that were correctly identified, measuring the model's ability to find all positive cases. F1-score was computed as the harmonic mean of precision and recall, providing a single metric that balances both measures and is particularly informative for imbalanced datasets.

For models capable of producing probability estimates (all except K-Nearest Neighbors without probability calibration), additional probabilistic metrics were computed. The area under the receiver operating characteristic curve (ROC-AUC) was calculated by varying the classification threshold and plotting the true positive rate against the false positive rate, with the area under this curve providing a threshold-independent measure of classification performance. Average precision, computed from the precision-recall curve, provides a summary measure that is particularly informative for imbalanced datasets as it focuses on the performance on the minority (active) class. Confusion matrices were constructed for each model, tabulating true positives, true negatives, false positives, and false negatives to provide detailed insight into the types of errors made by each classifier.

Feature importance analysis was performed for tree-based models (Random Forest, XGBoost, and Gradient Boosting) using the built-in feature importance measures that quantify the average contribution of each feature to the model's predictions across all trees. For Random Forest, importance was measured as the mean decrease in impurity (Gini importance) weighted by the number of samples reaching each node. For gradient boosting methods, importance was measured as the average gain (improvement in loss function) contributed by each feature across all trees. The top 20 most important features were identified and visualized for each tree-based model to provide insights into which molecular descriptors and engineered features most strongly influence activity predictions.

## Statistical Analysis and Model Comparison

Model performance metrics were compared across all seven algorithms to identify the best performing approach and to understand the relative strengths of different learning paradigms on this dataset. ROC curves for all models were plotted on a single graph to enable visual comparison of their discrimination capabilities across different classification thresholds. The models were ranked by F1-score as the primary metric, given its appropriateness for moderately imbalanced classification problems where both precision and recall are important. Statistical significance of performance differences was assessed qualitatively by examining the magnitude of metric differences and the consistency of performance patterns across multiple evaluation measures. The best performing model was identified based on achieving the highest F1-score while also demonstrating strong performance across other complementary metrics such as accuracy, precision, recall, and ROC-AUC.

## Software Implementation and Reproducibility

All data processing, feature engineering, model training, and evaluation were implemented in Python 3.9.6 using established scientific computing libraries. Data manipulation and analysis utilized pandas (version 2.1.4) and NumPy (version 1.26.2). Machine learning algorithms were implemented using scikit-learn (version 1.3.2) for Logistic Regression, Random Forest, Gradient Boosting, SVM, K-Nearest Neighbors, and Naive Bayes, and XGBoost (version 2.0.3) for the XGBoost classifier. The SMOTE implementation was provided by imbalanced-learn (version 0.11.0). Model serialization for saving and loading trained models utilized joblib (version 1.3.2). Visualization of results was performed using Plotly (version 5.18.0), Seaborn (version 0.13.0), and Matplotlib (version 3.8.2). An interactive web-based dashboard for data exploration, model comparison, and real-time predictions was developed using Streamlit (version 1.29.0), providing a user-friendly interface for interacting with the trained models and exploring the dataset.

The complete analysis pipeline was organized into modular Python scripts: a preprocessing module handling data loading, cleaning, and feature engineering; a models module implementing all machine learning algorithms and evaluation metrics; a training script orchestrating the full model training pipeline; and a Streamlit application providing interactive visualization and prediction capabilities. All code was version controlled and documented with comprehensive docstrings explaining the purpose and parameters of each function. Random seeds were fixed throughout the analysis to ensure reproducibility of all results. The computational environment was managed using Python virtual environments with all dependencies specified in a requirements file, enabling exact replication of the software environment. All analyses were performed on a local computing environment running macOS, with training times ranging from seconds for simple models like Naive Bayes to several minutes for complex ensemble methods like Random Forest and XGBoost on the full training dataset.
